{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T08:53:50.315270Z",
     "start_time": "2022-10-17T08:53:50.305181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.container#notebook-container {width: 1300px}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.container#notebook-container {width: 1300px}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T07:11:58.490332Z",
     "start_time": "2022-10-13T07:11:58.407312Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Cairo\")  # alternatives: nbAgg\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.family\"] = \"sans-serif\"\n",
    "rcParams['font.sans-serif'] = ['DejaVu Sans Mono']\n",
    "rcParams[\"pdf.fonttype\"] = 42\n",
    "rcParams[\"ps.fonttype\"] = 42\n",
    "# plt.style.use('default')\n",
    "\n",
    "import seaborn\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool, RLock, freeze_support\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "freeze_support()  # for Windows support\n",
    "tqdm.set_lock(RLock())  # for managing output contention\n",
    "\n",
    "# Import data load and process functions\n",
    "from main_dycause_mp_new import dycause_causal_discover\n",
    "import utility_funcs.data_load_funcs as data_funcs\n",
    "\n",
    "# Import data and graph plot functions\n",
    "import utility_funcs.graph_draw as gd\n",
    "\n",
    "# Import modified interval process functions\n",
    "import dycause_lib.method_improves as meth_imp\n",
    "\n",
    "# Import root cause analysis functions\n",
    "from dycause_lib.rca import analyze_root, normalize_by_column, search_path, bfs, case_rca_backtrace\n",
    "from utility_funcs.evaluation_function import pr_stat, print_prk_acc, my_acc\n",
    "import utility_funcs.exp_result_analyze as exp_ana\n",
    "\n",
    "# For PCMCI functions\n",
    "import tigramite.data_processing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Helper Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T11:32:46.612641Z",
     "start_time": "2022-09-29T11:32:46.533317Z"
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def cross_validation(all_case_result, train_keys=['case1', 'case2', 'case3', 'case5', 'case6_host204', 'case6_host28'], verbose=False):\n",
    "    training_dict = {}\n",
    "    testing_dict = {}\n",
    "    for k in all_case_result:\n",
    "        if k in train_keys:\n",
    "            training_dict[k] = all_case_result[k]\n",
    "        else:\n",
    "            testing_dict[k] = all_case_result[k]\n",
    "    idx = exp_ana.print_case_avg_max_perf(training_dict, verbose=verbose)\n",
    "    data = []\n",
    "    for k in testing_dict:\n",
    "        try:\n",
    "            data.append([k, *testing_dict[k][idx]['prks'], np.mean(testing_dict[k][idx]['prks']), \n",
    "                         testing_dict[k][idx]['acc'], sum(testing_dict[k][idx]['time_info'].values())])\n",
    "        except Exception:\n",
    "            from IPython.core.debugger import set_trace\n",
    "            set_trace() # 断点位置\n",
    "    data.append(['case avg', *[np.mean([data[i][j+1] for i in range(len(data))]) for j in range(13)]])\n",
    "    if verbose:\n",
    "        print(tabulate(data, headers=['Case']+['PR@{}'.format(i+1) for i in range(10)]+['PR@Avg', 'Acc', 'Time'], floatfmt=\"#06.4f\"))\n",
    "    return idx, data\n",
    "\n",
    "\n",
    "def enum_fold_cross_val(ours_all_case_result, train_size=5):\n",
    "    each_fold_data = []\n",
    "    keys_list = sorted(list(ours_all_case_result.keys()))  # sorted in order to keep the same with different dict\n",
    "    rnd = len(ours_all_case_result) // train_size\n",
    "    for i in range(rnd):\n",
    "        _, data = cross_validation(ours_all_case_result, train_keys=keys_list[i*train_size:(i+1)*train_size], verbose=False)\n",
    "        each_fold_data.append(data)\n",
    "    final_data = []\n",
    "    for i in range(4):\n",
    "        final_data.append(each_fold_data[i][-1][1:])\n",
    "    final_avg = ['Avg'] + np.mean(final_data, axis=0).tolist()\n",
    "    final_std = ['Std'] + np.std(final_data, axis=0).tolist()\n",
    "    print(tabulate([final_avg, final_std], headers=['Metric'] + ['PR@{}'.format(i+1) for i in range(10)]+['PR@Avg', 'Acc', 'Time'], floatfmt=\"#06.4f\"))\n",
    "    for i in range(1, len(final_avg)):\n",
    "        print('{:.3g}±{:.3g}'.format(final_avg[i], final_std[i]), end=' ')\n",
    "    return final_avg, final_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T04:34:57.008733Z",
     "start_time": "2022-09-16T04:34:56.942960Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------- Domain Knowledge --------------------------------------\n",
    "only_source_cols = ['inBytes', 'inMulticast', 'inPackets',\n",
    "                    'tcpPkgInsegs', 'udpInDatagrams']\n",
    "only_target_cols = ['outBytes', 'outPackets',\n",
    "                    'tcpPkgOutsegs', 'tcpPkgRetranssegs',\n",
    "                    'udpOutDatagrams']\n",
    "\n",
    "def get_only_source_target_idx(only_source_cols, only_target_cols, col_list):\n",
    "    def find_match_ind(simple_column):\n",
    "        for i, c in enumerate(col_list):\n",
    "            if gd.remove_parenthesis(c) == simple_column:\n",
    "                return i\n",
    "#         assert False, f'Column {simple_column} not found in col_list!'\n",
    "        return -1\n",
    "    source_ids = [find_match_ind(i) for i in only_source_cols]\n",
    "    source_ids = [i for i in source_ids if i!=-1]\n",
    "    target_ids = [find_match_ind(i) for i in only_target_cols]\n",
    "    target_ids = [i for i in target_ids if i != -1]\n",
    "    return source_ids, target_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Drawing Column Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T08:45:33.151604Z",
     "start_time": "2022-10-17T08:45:32.955350Z"
    },
    "code_folding": [
     17,
     39
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2367621/781448323.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mcolumns_by_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns_d_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mcolor_info_by_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns_d_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_by_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mcolumns_by_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case7'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns_d_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gd' is not defined"
     ]
    }
   ],
   "source": [
    "#--------- Case 1,2,5,6 Net & Runtime Metrics ----------\n",
    "all_columns_d_1 = {\n",
    "    'eth0': ['(iface=eth0)inPackets', '(iface=eth0)inPercent', '(iface=eth0)inMulticast', \n",
    "             '(iface=eth0)outPercent', '(iface=eth0)outPackets',\n",
    "             '(iface=eth0)totalBytes', '(iface=eth0)totalPackets'],\n",
    "    'cpu': ['busy', 'iowait', 'softirq', 'nice', 'system', 'user', 'switches'], \n",
    "    'kernel': ['kernelFilesAllocated'], \n",
    "    'load': ['load1', 'load5'],\n",
    "    'memory': ['memBuffers', 'memCached', 'memShmem', 'memUsedPercent', 'memAvailablePercent'], \n",
    "    'tcp': ['retrans', 'tcpAbortOnTimeout', 'tcpDelayedACKLocked', 'tcpLostRetransmit', 'tcpPkgInsegs', 'tcpPkgOutsegs',\n",
    "    'tcpPkgRetranssegs', 'tcpTW'],\n",
    "    'socket': ['ssClosed', 'ssEstab', 'ssOrphaned', 'ssTimeWait'],\n",
    "    'udp': ['udpInDatagrams', 'udpNoPorts', 'udpOutDatagrams', 'udpIgnoreMulti_Diff']\n",
    "}\n",
    "# --------- Case 7 all_columns\n",
    "all_columns_d_3 = {\n",
    "    \"eth0\": [\n",
    "        \"(iface=eth0)inPercent\",\n",
    "        \"(iface=eth0)inPackets\",\n",
    "        \"(iface=eth0)outPercent\",\n",
    "        \"(iface=eth0)outPackets\",\n",
    "        \"(iface=eth0)totalBytes\",\n",
    "        \"(iface=eth0)totalPackets\",\n",
    "    ],\n",
    "    \"cpu\": [\"busy\", \"iowait\", \"softirq\", \"system\", \"user\", \"switches\"],\n",
    "    \"kernel\": [\"kernelFilesAllocated\"],\n",
    "    \"load\": [\"load1\", \"load5\"],\n",
    "    \"memory\": [\"memBuffers\", \"memCached\", \"memShmem\", \"memUsedPercent\", \"memAvailablePercent\"],\n",
    "    \"tcp\": [\n",
    "        \"retrans\",\n",
    "        \"tcpAbortOnTimeout\",\n",
    "        \"tcpDelayedACKLocked\",\n",
    "        \"tcpPkgInsegs\",\n",
    "        \"tcpPkgOutsegs\",\n",
    "        \"tcpTW\",\n",
    "    ],\n",
    "    \"socket\": [\"ssClosed\", \"ssEstab\", \"ssOrphaned\", \"ssTimeWait\"],\n",
    "    \"udp\": [\"udpInDatagrams\", \"udpNoPorts\", \"udpOutDatagrams\", \"udpIgnoreMulti_Diff\"],\n",
    "    \"disk\": [\n",
    "        \"(mount=/data00)df.statistics.used.percent\",\n",
    "        \"(mount=/data00)df.statistics.used\",\n",
    "        \"(mount=/data00)df.statistics.total\",\n",
    "        \"(mount=/data00)df.inodes.free.percent\",\n",
    "        \"(mount=/data00)df.bytes.free.percent\",\n",
    "        \"(mount=/)df.statistics.used.percent\",\n",
    "        \"(mount=/)df.statistics.used\",\n",
    "        \"(mount=/)df.statistics.total\",\n",
    "        \"(mount=/)df.bytes.free.percent\",\n",
    "    ],\n",
    "    \"diskio\": [\n",
    "        \"(device=sda)disk.io.write\",\n",
    "        \"(device=sda)disk.io.w_wait\",\n",
    "        \"(device=sda)disk.io.util\",\n",
    "        \"(device=sda)disk.io.read\",\n",
    "        \"(device=sda)disk.io.await\",\n",
    "        \"(device=sda)disk.io.read_bytes_Diff\",\n",
    "        \"(device=sda)disk.io.write_bytes_Diff\",\n",
    "        \"(device=sdb)disk.io.write\",\n",
    "        \"(device=sdb)disk.io.w_wait\",\n",
    "        \"(device=sdb)disk.io.util\",\n",
    "        \"(device=sdb)disk.io.read\",\n",
    "        \"(device=sdb)disk.io.await\",\n",
    "        \"(device=sdb)disk.io.read_bytes_Diff\",\n",
    "        \"(device=sdb)disk.io.write_bytes_Diff\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "columns_by_case = {}\n",
    "color_info_by_case = {}\n",
    "\n",
    "def extract_cols(d):\n",
    "    all_columns = []\n",
    "    for k, v in d.items():\n",
    "        all_columns += sorted(v)\n",
    "    return all_columns\n",
    "columns_by_case['case1'] = extract_cols(all_columns_d_1)\n",
    "color_info_by_case['case1'] = gd.get_node_colors(all_columns_d_1, columns_by_case['case1'])\n",
    "\n",
    "columns_by_case['case7'] = extract_cols(all_columns_d_3)\n",
    "color_info_by_case['case7'] = gd.get_node_colors(all_columns_d_3, columns_by_case['case7'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Method Parameter Search\n",
    "Example on one case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T02:25:29.919700Z",
     "start_time": "2022-10-13T02:25:29.858646Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "plot_out_dir = \"plot_out_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T08:27:40.524071Z",
     "start_time": "2022-09-27T08:27:40.208695Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "save_names = []\n",
    "dycause_input = pd.read_csv(os.path.join('sample_data', 'anomaly_host_metrics.csv'), index_col=0)\n",
    "dycause_input.index = pd.to_datetime(dycause_input.index)\n",
    "\n",
    "print(\"Data shape:\", dycause_input.shape)\n",
    "\n",
    "df = data_funcs.normalize_df(dycause_input)\n",
    "\n",
    "max_segment_len = None\n",
    "for sign in [0.01, 0.05, 0.1]:\n",
    "    for step in [30]:\n",
    "        for lag in [6, 7, 8]:\n",
    "            local_results_dy, dcc_dy, mat_dy, time_stat_dict_dy = dycause_causal_discover(\n",
    "                # Data params\n",
    "                dycause_input.to_numpy()[:, :],\n",
    "                # Granger interval based graph construction params\n",
    "                step=step,\n",
    "                significant_thres=sign,\n",
    "                lag=lag,  # must satisfy: step > 3 * lag + 1\n",
    "                adaptive_threshold=0.7,\n",
    "                use_multiprocess=True,\n",
    "                max_workers=10,\n",
    "                max_segment_len=max_segment_len,\n",
    "                # Debug_params\n",
    "                verbose=2,\n",
    "                runtime_debug=True,\n",
    "            )\n",
    "\n",
    "            print(time_stat_dict_dy['Construct-Impact-Graph-Phase'])\n",
    "\n",
    "            # Use the timezone in my location.\n",
    "            local_tz = datetime.timezone(datetime.timedelta(hours=8))\n",
    "            time_str = datetime.datetime.now(local_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "            fname = os.path.join(\"temp_results\", casename, f\"exp_rets_{time_str}.pkl\")\n",
    "            save_names.append(fname)\n",
    "            # print(\"Saving results to:\", fname)\n",
    "            os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "            with open(fname, \"wb\") as f:\n",
    "                pickle.dump({\n",
    "                    \"local_results\": local_results_dy,\n",
    "                    \"dcc\": dcc_dy,\n",
    "                    \"mat\": mat_dy,\n",
    "                    \"time_stat_dict\": time_stat_dict_dy,\n",
    "                    \"step\": step,\n",
    "                    \"lag\": lag,\n",
    "                    \"sign\": sign\n",
    "                }, f)\n",
    "print(save_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* `all_case_input` is the dict for the host metrics data, diagnosis entry and the root causes.\n",
    "* `save_names_1_7_nine` is the exp_rets results generated using previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_case_rca_exp_res = {}\n",
    "for case_num in tqdm(['case3']):\n",
    "    df_path = df_path_dict[case_num]\n",
    "    dycause_input = pd.read_csv(df_path, index_col=0)\n",
    "    dycause_input.index = pd.to_datetime(dycause_input.index)\n",
    "    hint_thres = 30 # for case 1-6: 250, for case 7-25: 30\n",
    "    ntop=80\n",
    "    all_case_rca_exp_res[case_num] = []\n",
    "    # ---- Fill the following dict with save exp_rets in dycause_lib/ours_exp_logs.py\n",
    "    for fname in save_names_1_7_nine[case_num][-1:]:\n",
    "        with open(os.path.join(\"temp_results\", exp_dir_dict[case_num], fname), \"rb\") as f:\n",
    "            exp_ret = pickle.load(f)\n",
    "        time_info = {'DyCause': exp_ret[\"time_stat_dict\"][\"Construct-Impact-Graph-Phase\"]}\n",
    "        exp_ret['time_info'] = time_info\n",
    "        meth_imp.build_intervals_special(exp_ret['local_results'], exp_ret['sign'], dycause_input.shape[0], \n",
    "                                         exp_ret['step'], \n",
    "                                         dycause_input.shape[1])\n",
    "        exp_ret['dcc_special'] = meth_imp.generate_DCC(exp_ret['local_results'], dycause_input.shape[0], \n",
    "                                                       dycause_input.shape[1], interval_key=\"intervals_special\")\n",
    "    \n",
    "\n",
    "        columns = columns_by_case[case_num]\n",
    "        N = len(columns)\n",
    "        tic = time.time()\n",
    "        data_idx_to_graph_idx, graph_idx_to_data_idx = gd.construct_d2g_map_dict(\n",
    "            list(dycause_input.columns), columns, print_info=False\n",
    "        )\n",
    "        exp_ret[\"filtered_dcc\"] = gd.filter_dcc(\n",
    "            exp_ret[\"dcc\"], N, graph_idx_to_data_idx\n",
    "        )\n",
    "        exp_ret[\"filtered_dcc_special\"] = gd.filter_dcc(\n",
    "            exp_ret[\"dcc_special\"], N, graph_idx_to_data_idx\n",
    "        )\n",
    "        filtered_df = gd.filter_df(dycause_input, graph_idx_to_data_idx, N)\n",
    "        # hints from anomaly dcc\n",
    "        hints = meth_imp.get_dir_hints_from_dcc(exp_ret[\"filtered_dcc\"], N, hint_thres)\n",
    "#         hints = None\n",
    "#         exp_ret['hint_thres'] = hint_thres\n",
    "        # merge hints from normal profile\n",
    "        # hints = hints + normal_profile_hints_filtered\n",
    "        # hints = meth_imp.remove_double_direction_hints(hints)\n",
    "\n",
    "        o_s_ids, o_t_ids = get_only_source_target_idx(\n",
    "            only_source_cols, only_target_cols, columns\n",
    "        )\n",
    "#         o_s_ids, o_t_ids = None, None\n",
    "\n",
    "        exp_ret['ntop'] = ntop\n",
    "#     # for ntop in [80]:\n",
    "        mat, candidates = meth_imp.global_thresholding(\n",
    "            exp_ret[\"filtered_dcc_special\"],\n",
    "            N,\n",
    "            normal_axis=\"none\",\n",
    "            ntop=ntop,\n",
    "            only_source_ids=o_s_ids,\n",
    "            only_target_ids=o_t_ids,\n",
    "            hints=hints,\n",
    "            return_candidates=True\n",
    "        )\n",
    "        toc = time.time() - tic\n",
    "        exp_ret['mat'] = mat\n",
    "        exp_ret['candidates'] = candidates\n",
    "        exp_ret['time_info']['Graph'] = toc\n",
    "\n",
    "        tic = time.time()\n",
    "        # corr_res_cond_shuffled, corr_res_cond = calculate_CMIknn_shuffled(mat, filtered_df)\n",
    "        corr_res_cond_shuffled, corr_res_cond = meth_imp.calculate_CMIknn_shuffled_mp(exp_ret['mat'], \n",
    "                                                                                      filtered_df.iloc[-120:, :], \n",
    "                                                                                      max_workers=10, \n",
    "                                                                                      verbose=True)\n",
    "        # corr_res_cond_shuffled = meth_imp.calculate_parcorr(mat, filtered_df)\n",
    "        toc = time.time() - tic\n",
    "        exp_ret['corr_res_cond_shuffled'] = corr_res_cond_shuffled\n",
    "#         exp_ret['corr_res_cond_shuffled'] = None\n",
    "        exp_ret['time_info']['Cond'] = toc\n",
    "        \n",
    "        k = case_num\n",
    "        entry = all_case_input[k]['entry']\n",
    "        root_causes = all_case_input[k]['root_causes']\n",
    "\n",
    "        exps = meth_imp.search_rca_backtrace_params(exp_ret['mat'], \n",
    "                                                    exp_ret['candidates'], \n",
    "                                                    exp_ret['corr_res_cond_shuffled'], \n",
    "                                                    entry, root_causes, filtered_df, \n",
    "                                                    exp_ret['time_info'],\n",
    "                                                    verbose=False, comp_func=lambda x,p:x>p)\n",
    "        for d in exps:\n",
    "            d['step'] = exp_ret['step']\n",
    "            d['lag'] = exp_ret['lag']\n",
    "            d['sign'] = exp_ret['sign']\n",
    "            d['hint_thres'] = hint_thres\n",
    "            d['ntop'] = ntop\n",
    "        all_case_rca_exp_res[case_num].extend(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameter search results\n",
    "with open(f'temp_results/all_case_rca_exp_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_case_rca_exp_res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perf Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-10T12:43:22.124651Z",
     "start_time": "2022-10-10T12:43:16.440383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the parameter search results for each case\n",
    "with open(f'temp_results/all_case_rca_exp_results.pkl', 'rb') as f:\n",
    "    all_case_rca_exp_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T11:42:02.327705Z",
     "start_time": "2022-09-29T11:42:01.353150Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric      PR@1    PR@2    PR@3    PR@4    PR@5    PR@6    PR@7    PR@8    PR@9    PR@10    PR@Avg     Acc     Time\n",
      "--------  ------  ------  ------  ------  ------  ------  ------  ------  ------  -------  --------  ------  -------\n",
      "Avg       0.0658  0.0921  0.1272  0.1645  0.2342  0.2829  0.3276  0.3855  0.4263   0.4789    0.2585  0.7702  74.9261\n",
      "Std       0.0436  0.0228  0.0375  0.0218  0.0142  0.0151  0.0269  0.0180  0.0337   0.0411    0.0037  0.0117   5.8263\n",
      "0.0658±0.0436 0.0921±0.0228 0.127±0.0375 0.164±0.0218 0.234±0.0142 0.283±0.0151 0.328±0.0269 0.386±0.018 0.426±0.0337 0.479±0.0411 0.259±0.00373 0.77±0.0117 74.9±5.83 "
     ]
    }
   ],
   "source": [
    "cross_val_avg_std = {}\n",
    "cross_val_avg_std['ours'] = enum_fold_cross_val(all_case_rca_exp_res, train_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DyCause\n",
    "Notes:\n",
    "* `save_names` is the exp_rets list as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T08:40:17.209355Z",
     "start_time": "2022-08-26T08:40:17.167261Z"
    }
   },
   "outputs": [],
   "source": [
    "dycause_all_case_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T00:58:39.915271Z",
     "start_time": "2022-08-30T00:58:39.752124Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dycause_rca_search(exp_ret, columns, entry, root_causes, filtered_df):\n",
    "    \n",
    "    dycause_results = []\n",
    "    for thres in tqdm(np.arange(0.1, 1.0, 0.1)):\n",
    "        tic = time.time()\n",
    "        mat = adaptive_thresholding(\n",
    "            exp_ret[\"filtered_dcc\"], thres, len(columns)\n",
    "        )\n",
    "        toc = time.time() - tic\n",
    "        dcc_sum = {}\n",
    "        for k in exp_ret[\"filtered_dcc\"]:\n",
    "            dcc_sum[k] = float(np.sum(exp_ret[\"filtered_dcc\"][k]))\n",
    "        \n",
    "        exp_rets = backtrace_param_search(mat, entry, root_causes, filtered_df)\n",
    "        \n",
    "        for d in exp_rets:\n",
    "            d['thres'] = thres\n",
    "            d['mat'] = mat\n",
    "            d['dcc_sum'] = dcc_sum\n",
    "            d['time_info']['Adaptive'] = toc\n",
    "            d['time_info']['DyCause'] = time_info['DyCause']\n",
    "        dycause_results.extend(exp_rets)\n",
    "    return dycause_results\n",
    "\n",
    "for case_num in tqdm([1]):\n",
    "    case_str = f'case{case_num}'\n",
    "    print('{:-^80}'.format(f' {case_str} '))\n",
    "    dycause_input = pd.read_csv(os.path.join(data_out_dir, f'dycause_input_{case_num}.csv'),\n",
    "                            index_col=0)\n",
    "    dycause_input.index = pd.to_datetime(dycause_input.index)\n",
    "    fname = \"temp_results/\" + save_names[case_num]\n",
    "    with open(fname, \"rb\") as f:\n",
    "        exp_ret = pickle.load(f)\n",
    "    time_info = {'DyCause': exp_ret[\"time_stat_dict\"][\"Construct-Impact-Graph-Phase\"]}\n",
    "#     print('DyCause Time:', f'{exp_ret[\"time_stat_dict\"][\"Construct-Impact-Graph-Phase\"]:.4f}')\n",
    "\n",
    "    meth_imp.build_intervals_special(exp_ret['local_results'], exp_ret['sign'], dycause_input.shape[0], \n",
    "                                     exp_ret['step'], dycause_input.shape[1])\n",
    "    exp_ret['dcc_special'] = meth_imp.generate_DCC(exp_ret['local_results'], dycause_input.shape[0], \n",
    "                                                   dycause_input.shape[1], interval_key=\"intervals_special\")\n",
    "    data_idx_to_graph_idx, graph_idx_to_data_idx  = gd.construct_d2g_map_dict(list(dycause_input.columns), \n",
    "                                                                      columns_by_case[case_str])\n",
    "    exp_ret[\"filtered_dcc\"] = gd.filter_dcc(exp_ret[\"dcc\"], len(columns_by_case[case_str]), graph_idx_to_data_idx)\n",
    "    filtered_df = gd.filter_df(dycause_input, graph_idx_to_data_idx, len(columns_by_case[case_str]))\n",
    "    entry = get_case_entry(case_num)\n",
    "    root_causes = get_case_root_causes(case_num)\n",
    "    dycause_results = dycause_rca_search(exp_ret, columns_by_case[case_str], entry, root_causes, filtered_df)\n",
    "#     dycause_all_case_result[case_num] = dycause_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T11:43:37.434392Z",
     "start_time": "2022-09-29T11:43:37.231772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric      PR@1    PR@2    PR@3    PR@4    PR@5    PR@6    PR@7    PR@8    PR@9    PR@10    PR@Avg     Acc     Time\n",
      "--------  ------  ------  ------  ------  ------  ------  ------  ------  ------  -------  --------  ------  -------\n",
      "Avg       0.1579  0.1447  0.1557  0.1711  0.1868  0.2171  0.2618  0.2789  0.2895   0.3197    0.2183  0.4166  36.2762\n",
      "Std       0.0744  0.0543  0.0293  0.0360  0.0362  0.0317  0.0549  0.0426  0.0448   0.0606    0.0418  0.1352   1.0182\n",
      "0.158±0.0744 0.145±0.0543 0.156±0.0293 0.171±0.036 0.187±0.0362 0.217±0.0317 0.262±0.0549 0.279±0.0426 0.289±0.0448 0.32±0.0606 0.218±0.0418 0.417±0.135 36.3±1.02 "
     ]
    }
   ],
   "source": [
    "cross_val_avg_std['dycause'] = enum_fold_cross_val(dycause_all_case_result, train_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudRanger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T06:14:18.893561Z",
     "start_time": "2022-08-30T06:14:18.049868Z"
    }
   },
   "outputs": [],
   "source": [
    "from cloud_ranger_lib.get_link_matrix import build_graph_pc\n",
    "import cloud_ranger_lib.cloud_ranger as cloud_ranger\n",
    "from utility_funcs.pearson import calc_pearson\n",
    "\n",
    "\n",
    "def main_cloudranger(df, frontend, true_root_cause, pc_aggregate=5,\n",
    "    pc_alpha=0.1, beta=0.3, \n",
    "    rho=0.2, testrun_round=5, verbose=False):\n",
    "    '''\n",
    "    Params:\n",
    "        df: input dataframe, shape [T, N]\n",
    "    '''\n",
    "    tic = time.time()\n",
    "    data = df.to_numpy()\n",
    "    T, N = data.shape\n",
    "    data = np.array([cloud_ranger.aggregate(row, pc_aggregate) for row in data.T]) # shape: [N, T]\n",
    "    rela = calc_pearson(data, method=\"numpy\", zero_diag=False)\n",
    "    access = build_graph_pc(data, alpha=pc_alpha)\n",
    "    pc_toc = time.time() - tic\n",
    "    \n",
    "    prks = []\n",
    "    acc = []\n",
    "    tic = time.time()\n",
    "    for i in range(testrun_round):\n",
    "        rank, P, M = cloud_ranger.relaToRank(\n",
    "            rela, access, 10, frontend, beta=beta, rho=rho, print_trace=False\n",
    "        )\n",
    "        if verbose:\n",
    "            for j in range(10):\n",
    "                print(rank[j], end=\", \")\n",
    "            print(\"\")\n",
    "        prks.append(pr_stat(rank, true_root_cause, 10))\n",
    "        acc.append(my_acc(rank, true_root_cause, n=N))\n",
    "    toc = time.time() - tic\n",
    "    toc = toc / testrun_round\n",
    "    prks = np.mean(prks, axis=0)\n",
    "    acc = np.mean(acc)\n",
    "    if verbose:\n",
    "        print_prk_acc(prks, acc)\n",
    "    return prks, acc, {'random_walk_time': toc, 'pc_time': pc_toc}, {'access': access, 'rank': rank}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T01:54:58.765067Z",
     "start_time": "2022-08-30T01:54:58.697455Z"
    }
   },
   "outputs": [],
   "source": [
    "cr_all_case_result = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CloudRanger on One Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T02:38:03.705739Z",
     "start_time": "2022-08-31T02:33:20.054417Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "pool = mp.Pool(10)\n",
    "results = []\n",
    "for pc_agg in [5, 10, 20]:\n",
    "    for pc_a in [0.05, 0.1, 0.15]:\n",
    "        for beta in [0.2, 0.4, 0.6]:\n",
    "            for rho in [0.2, 0.4, 0.6]:\n",
    "                results.append(pool.apply_async(main_cloudranger, (filtered_df, entry+1, [i+1 for i in root_causes],\n",
    "                                                                   pc_agg, pc_a, beta, rho)))\n",
    "pbar = tqdm(total=len(results))\n",
    "for res in results:\n",
    "    res.wait() # wait until it completes\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "pool.close()\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T02:40:18.032970Z",
     "start_time": "2022-08-31T02:40:17.947479Z"
    }
   },
   "outputs": [],
   "source": [
    "cr_exp_rets = [{'prks': r.get()[0], 'acc': r.get()[1], 'time_info': r.get()[2], 'others': r.get()[3]} for r in results]\n",
    "idx = 0\n",
    "for pc_agg in [5, 10, 20]:\n",
    "    for pc_a in [0.05, 0.1, 0.15]:\n",
    "        for beta in [0.2, 0.4, 0.6]:\n",
    "            for rho in [0.2, 0.4, 0.6]:\n",
    "                cr_exp_rets[idx].update({\n",
    "                    'pc_agg': pc_agg,\n",
    "                    'pc_a': pc_a,\n",
    "                    'beta': beta,\n",
    "                    'rho': rho\n",
    "                })\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* `cr_all_case_result` is the dict storing the CloudRanger search results of all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T11:45:18.572305Z",
     "start_time": "2022-09-29T11:45:18.497139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric      PR@1    PR@2    PR@3    PR@4    PR@5    PR@6    PR@7    PR@8    PR@9    PR@10    PR@Avg     Acc     Time\n",
      "--------  ------  ------  ------  ------  ------  ------  ------  ------  ------  -------  --------  ------  -------\n",
      "Avg       0.0105  0.0211  0.0307  0.0770  0.0776  0.1239  0.1287  0.1392  0.1492   0.1558    0.0914  0.6536  60.9526\n",
      "Std       0.0182  0.0207  0.0228  0.0387  0.0340  0.0179  0.0191  0.0249  0.0266   0.0195    0.0147  0.0126  11.1873\n",
      "0.0105±0.0182 0.0211±0.0207 0.0307±0.0228 0.077±0.0387 0.0776±0.034 0.124±0.0179 0.129±0.0191 0.139±0.0249 0.149±0.0266 0.156±0.0195 0.0914±0.0147 0.654±0.0126 61±11.2 "
     ]
    }
   ],
   "source": [
    "cross_val_avg_std['cloudranger'] = enum_fold_cross_val(cr_all_case_result, train_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T07:00:13.501440Z",
     "start_time": "2022-09-06T07:00:13.425718Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('temp_results/cloudranger/case1_cloudranger_exp_results.pkl', 'rb') as f:\n",
    "    cr_case1_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCMCI RCA\n",
    "* PCMCI with Backtrace RCA\n",
    "* PCMCI with CloudRanger second-order random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-10T10:13:37.364166Z",
     "start_time": "2022-09-10T10:13:37.302230Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from baselines.main_pcmci import run_pcmci, pcmci_build_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-10T10:13:39.077227Z",
     "start_time": "2022-09-10T10:13:39.018249Z"
    }
   },
   "outputs": [],
   "source": [
    "pcmci_all_case_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-10T10:28:09.002585Z",
     "start_time": "2022-09-10T10:13:44.165909Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case in ['case1']:\n",
    "    entry = all_case_input[case]['entry']\n",
    "    root_causes = all_case_input[case]['root_causes']\n",
    "    filtered_df = all_case_input[case]['filtered_df']\n",
    "\n",
    "    # ---- Multiprocess run PCMCI Step 1 Conduct Tests ----\n",
    "    import multiprocessing as mp\n",
    "    pool = mp.Pool(10)\n",
    "    pcmci_exp_rets = []\n",
    "    results = []\n",
    "    for pc_alpha in [0.01, 0.05, 0.1]:\n",
    "        for lag in [6, 7, 8]:\n",
    "            pcmci_exp_rets.append({\n",
    "                'pc_alpha': pc_alpha,\n",
    "                'lag': lag\n",
    "            })\n",
    "            results.append(pool.apply_async(run_pcmci, (filtered_df.to_numpy()[:, :], lag, pc_alpha, 'cmiknn')))\n",
    "    pbar = tqdm(total=len(pcmci_exp_rets))\n",
    "    for res in results:\n",
    "        res.wait() # wait until it completes\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "\n",
    "    for i, d in enumerate(pcmci_exp_rets):\n",
    "        try:\n",
    "            pcmci, pcmci_res, toc = results[i].get()\n",
    "            d['pcmci'] = pcmci\n",
    "            d['pcmci_res'] = pcmci_res\n",
    "            d['pcmci_time'] = toc\n",
    "        except ValueError as e:\n",
    "            d['pcmci'] = None\n",
    "            d['pcmci_res'] = None\n",
    "            d['pcmci_time'] = None\n",
    "\n",
    "\n",
    "    # ---- single process PCMCI ----\n",
    "    # pcmci_exp_rets = []\n",
    "    # idx = 0\n",
    "    # pbar = tqdm(total=9)\n",
    "    # for pc_alpha in [0.01, 0.05, 0.1]:\n",
    "    #     for lag in [5, 8, 10]:\n",
    "    #         tic = time.time()\n",
    "    #         try:\n",
    "    #             pcmci_res = run_pcmci(filtered_df.to_numpy()[:, :], pc_alpha=pc_alpha, tau_max=lag, verbosity=0)\n",
    "    #         except ValueError as e:\n",
    "    #             pcmci_res = None\n",
    "    #         toc = time.time()\n",
    "    #         pcmci_exp_rets.append({\n",
    "    #             \"pc_alpha\": pc_alpha, \n",
    "    #             'tau_max': lag,\n",
    "    #             \"pcmci_res\": pcmci_res,\n",
    "    #             \"pcmci_time\": toc-tic\n",
    "    #         })\n",
    "    #         pbar.update(1)\n",
    "    # pbar.close()\n",
    "\n",
    "    print('If valid results:', [d['pcmci_res'] is not None for d in pcmci_exp_rets])\n",
    "\n",
    "    # ---- PCMCI Step 2 Build Graph ----\n",
    "    pcmci_adj_res = []\n",
    "    for i in range(len(pcmci_exp_rets)):\n",
    "        tic = time.time()\n",
    "        if pcmci_exp_rets[i]['pcmci'] != None:\n",
    "            g = pcmci_build_g(pcmci_exp_rets[i]['pcmci'], pcmci_exp_rets[i]['pcmci_res'], pcmci_exp_rets[i]['pc_alpha'])\n",
    "            adj = nx.to_numpy_matrix(g)\n",
    "        else:\n",
    "            adj = None\n",
    "    #         print(adj)\n",
    "        toc = time.time()\n",
    "        pcmci_adj_res.append({\n",
    "            'adj': adj,\n",
    "            'pcmci_time_2': toc-tic\n",
    "        })\n",
    "        pcmci_adj_res[-1].update(pcmci_exp_rets[i])\n",
    "\n",
    "\n",
    "    # ---- single process version ----\n",
    "    # pcmci_rca_exp_results = []\n",
    "    # for d in tqdm(pcmci_adj_res):\n",
    "    #     if d['adj'] is not None:\n",
    "    #         exp_rets = backtrace_param_search(d['adj'], entry, root_causes, filtered_df)\n",
    "\n",
    "    #         for _d in exp_rets:\n",
    "    #             _d['time_info']['pcmci_time'] = d['pcmci_time']\n",
    "    #             _d['time_info']['pcmci_time_2'] = d['pcmci_time_2']\n",
    "    #             _d['adj'] = d['adj']\n",
    "    #             _d['pc_alpha'] = d['pc_alpha']\n",
    "    #             _d['lag'] = d['lag']\n",
    "    #         pcmci_rca_exp_results.extend(exp_rets)\n",
    "    #     else:\n",
    "    #         pcmci_rca_exp_results.extend([{'prks': None, 'acc': None} for _ in range(135)])\n",
    "\n",
    "    # ---- multiple processes version ----\n",
    "    pcmci_rca_exp_results = []\n",
    "    import multiprocessing as mp\n",
    "    pool = mp.Pool(10)\n",
    "    pool_results = []\n",
    "    for d in pcmci_adj_res:\n",
    "        if d['adj'] is not None:\n",
    "            # ---- backtrace RCA ----\n",
    "            pool_results.append(pool.apply_async(\n",
    "                meth_imp.backtrace_param_search, (d['adj'], entry, root_causes, filtered_df)))\n",
    "            # ---- Random Walk RCA ----\n",
    "#             pool_results.append(pool.apply_async(\n",
    "#                 pcmci_randwalk_params_search, (filtered_df.to_numpy().T, d['adj'].tolist(), entry+1, [i+1 for i in root_causes])))\n",
    "        else:\n",
    "            pool_results.append(None)\n",
    "    pool_futures = [i for i in pool_results if i is not None]\n",
    "    pbar = tqdm(total=len(pool_futures))\n",
    "    for fut in pool_futures:\n",
    "        fut.wait() # wait until it completes\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    for i, d in enumerate(pcmci_adj_res):\n",
    "        if d['adj'] is not None:\n",
    "            try:\n",
    "                exp_rets = pool_results[i].get()\n",
    "                for _d in exp_rets:\n",
    "                    _d['time_info']['pcmci_time'] = d['pcmci_time']\n",
    "                    _d['time_info']['pcmci_time_2'] = d['pcmci_time_2']\n",
    "                    _d['adj'] = d['adj']\n",
    "                    _d['pc_alpha'] = d['pc_alpha']\n",
    "                    _d['lag'] = d['lag']\n",
    "                pcmci_rca_exp_results.extend(exp_rets)\n",
    "            except ValueError as e:\n",
    "                pcmci_rca_exp_results.extend([{'prks': None, 'acc': None} for _ in range(45)])\n",
    "        else:\n",
    "            pcmci_rca_exp_results.extend([{'prks': None, 'acc': None} for _ in range(45)])\n",
    "\n",
    "    pcmci_all_case_result[case] = pcmci_rca_exp_results\n",
    "#     with open(f'temp_results/pcmci/{case}_pcmci2_exp_results_new.pkl', 'wb') as f:\n",
    "#         pickle.dump(pcmci_rca_exp_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T11:45:54.160387Z",
     "start_time": "2022-09-29T11:45:54.074568Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric      PR@1    PR@2    PR@3    PR@4    PR@5    PR@6    PR@7    PR@8    PR@9    PR@10    PR@Avg     Acc     Time\n",
      "--------  ------  ------  ------  ------  ------  ------  ------  ------  ------  -------  --------  ------  -------\n",
      "Avg       0.0625  0.1016  0.1094  0.1797  0.1875  0.2250  0.2687  0.3031  0.3281   0.3625    0.2128  0.7015  29.9527\n",
      "Std       0.0000  0.0341  0.0308  0.0524  0.0364  0.0293  0.0187  0.0223  0.0185   0.0198    0.0240  0.0444   4.8446\n",
      "0.0625±0 0.102±0.0341 0.109±0.0308 0.18±0.0524 0.188±0.0364 0.225±0.0293 0.269±0.0187 0.303±0.0223 0.328±0.0185 0.363±0.0198 0.213±0.024 0.701±0.0444 30±4.84 "
     ]
    }
   ],
   "source": [
    "cross_val_avg_std['pcmci bt'] = enum_fold_cross_val(pcmci_all_case_result, train_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T11:46:26.410875Z",
     "start_time": "2022-09-29T11:46:26.311022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric      PR@1    PR@2    PR@3    PR@4    PR@5    PR@6    PR@7    PR@8    PR@9    PR@10    PR@Avg     Acc     Time\n",
      "--------  ------  ------  ------  ------  ------  ------  ------  ------  ------  -------  --------  ------  -------\n",
      "Avg       0.0156  0.0781  0.0859  0.1289  0.1328  0.1797  0.2406  0.2594  0.3250   0.3656    0.1812  0.6677  32.2557\n",
      "Std       0.0271  0.0271  0.0348  0.0231  0.0173  0.0317  0.0185  0.0285  0.0515   0.0526    0.0117  0.0335   3.6727\n",
      "0.0156±0.0271 0.0781±0.0271 0.0859±0.0348 0.129±0.0231 0.133±0.0173 0.18±0.0317 0.241±0.0185 0.259±0.0285 0.325±0.0515 0.366±0.0526 0.181±0.0117 0.668±0.0335 32.3±3.67 "
     ]
    }
   ],
   "source": [
    "cross_val_avg_std['pcmci rw'] = enum_fold_cross_val(pcmci_all_case_result, train_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effcause-env",
   "language": "python",
   "name": "effcause-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "500px",
    "width": "383px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "569.7px",
    "left": "68.2px",
    "top": "110.325px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}